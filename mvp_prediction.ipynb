{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375e3978",
   "metadata": {},
   "source": [
    "# NBA MVP Prediction Using Machine Learning\n",
    "## Project Overview\n",
    "This notebook demonstrates how to predict NBA Most Valuable Player (MVP) winners using machine learning techniques. We'll analyze historical NBA player statistics to identify patterns and build classification models that can predict whether a player is likely to win the MVP award.\n",
    "\n",
    "#### Methodology:\n",
    "1. **Data Collection**: Load NBA player statistics, MVP history, and advanced metrics like RAPTOR\n",
    "2. **Feature Engineering**: Select relevant statistical features that may predict MVP status\n",
    "3. **Model Training**: Train multiple classification models (Logistic Regression, Random Forest, XGBoost)\n",
    "4. **Model Evaluation**: Compare model performance using classification metrics and ROC curves\n",
    "5. **Interpretability**: Visualize feature importance and use SHAP values to understand model decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dc08e9",
   "metadata": {},
   "source": [
    "### Library Imports\n",
    "\n",
    "This code imports the essential Python libraries for our NBA MVP prediction:\n",
    "\n",
    "- **Data manipulation**: pandas (pd) and numpy (np) for data handling\n",
    "- **Visualization**: matplotlib for plotting results, shap for model interpretability\n",
    "- **Machine learning**: \n",
    "  - sklearn for preprocessing, model selection, and evaluation metrics\n",
    "  - Various classifiers: LogisticRegression, RandomForestClassifier, and XGBoost\n",
    "  - Metrics for model evaluation: classification_report, ROC curves, confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6560847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve, \n",
    "    make_scorer, f1_score, roc_auc_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a163ae",
   "metadata": {},
   "source": [
    "### Data Loading and Preparation\n",
    "\n",
    "This cell loads and prepares the NBA data from multiple sources:## Data Loading and Preparation\n",
    "\n",
    "This code loads and merges multiple datasets:\n",
    "\n",
    "1. **MVP History**: Historical data of NBA MVP winners\n",
    "2. **NBA Dataset**: Player statistics and performance metrics\n",
    "3. **RAPTOR Metrics**: Advanced player analytics from FiveThirtyEight (both historical and modern)\n",
    "\n",
    "The data preparation steps include:\n",
    "- Standardizing season format\n",
    "- Creating an MVP target variable (1 = MVP winner, 0 = not MVP)\n",
    "- Merging datasets on player name and season\n",
    "- Handling missing values using median imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70692480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "mvp = pd.read_csv(\"data/mvp_history.csv\", skiprows=1, usecols=[\"Season\", \"Player\"])\n",
    "mvp[\"season\"] = mvp[\"Season\"].str.split(\"-\", expand=True)[0].astype(int) + 1\n",
    "mvp = mvp.rename(columns={\"Player\": \"player\"})[[\"season\", \"player\"]].assign(MVP=1)\n",
    "\n",
    "bb = pd.read_csv(\"data/NBA_Dataset.csv\")\n",
    "rap_h = pd.read_csv(\"data/historical_RAPTOR_by_player.csv\")\n",
    "rap_m = pd.read_csv(\"data/modern_RAPTOR_by_player.csv\")\n",
    "rap_cols = [\"player_name\", \"season\", \"raptor_offense\", \"raptor_defense\", \"raptor_total\"]\n",
    "rap = pd.concat([rap_h[rap_cols], rap_m[rap_cols]], ignore_index=True)\n",
    "rap = rap.drop_duplicates(subset=[\"player_name\", \"season\"]).rename(columns={\"player_name\": \"player\"})\n",
    "\n",
    "df = bb.merge(rap, on=[\"season\", \"player\"], how=\"left\").merge(mvp, on=[\"season\", \"player\"], how=\"left\")\n",
    "df[\"MVP\"] = df[\"MVP\"].fillna(0).astype(int)\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0823dd8",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "This code selects the most relevant features for MVP prediction:\n",
    "\n",
    "- **Box Score Stats**: Points, rebounds, and assists per game\n",
    "- **Advanced Metrics**: Win shares (ws), Box Plus/Minus (bpm), Value Over Replacement Player (vorp)\n",
    "- **Team Success**: Win-loss percentage\n",
    "- **RAPTOR Metrics**: Offensive, defensive, and total RAPTOR scores\n",
    "\n",
    "These features capture individual performance, team success, and advanced analytics - all factors that typically influence MVP voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f32da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "features = [\n",
    "    'ws', 'bpm', 'vorp', 'win_loss_pct',\n",
    "    'pts_per_g', 'trb_per_g', 'ast_per_g',\n",
    "    'raptor_offense', 'raptor_defense', 'raptor_total'\n",
    "]\n",
    "X = df[features]\n",
    "y = df[\"MVP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3689b9b6",
   "metadata": {},
   "source": [
    "### Data Splitting and Model Training\n",
    "\n",
    "This code:\n",
    "\n",
    "1. **Splits the data** into training (80%) and testing (20%) sets\n",
    "   - Stratifies by MVP status to ensure balanced representation\n",
    "   - Uses random_state=42 for reproducibility\n",
    "\n",
    "2. **Trains three different models**:\n",
    "   - **Logistic Regression**: With StandardScaler preprocessing\n",
    "   - **Random Forest**: 300 trees with depth limited to 8\n",
    "   - **XGBoost**: Gradient boosting with 300 estimators and 0.05 learning rate\n",
    "\n",
    "Each model approaches the classification problem differently:\n",
    "- Logistic regression is linear and interpretable\n",
    "- Random forest handles non-linear relationships and reduces overfitting\n",
    "- XGBoost is powerful for structured data and usually achieves strong performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Train models\n",
    "models = {\n",
    "    \"LogisticRegression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lr\", LogisticRegression(max_iter=2000, random_state=42))\n",
    "    ]).fit(X_tr, y_tr),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=300, max_depth=8, random_state=42, n_jobs=1\n",
    "    ).fit(X_tr, y_tr),\n",
    "    \"XGBoost\": xgb.XGBClassifier(\n",
    "        n_estimators=300, learning_rate=0.05,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42, n_jobs=1\n",
    "    ).fit(X_tr, y_tr)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2c1b9",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "This code evaluates each model's performance on the test set using:\n",
    "\n",
    "1. **Classification Report**: Shows precision, recall, F1-score, and support\n",
    "2. **ROC AUC Score**: Measures the model's ability to distinguish between MVP and non-MVP players\n",
    "3. **Confusion Matrix**: Displays true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP)\n",
    "\n",
    "These metrics help us understand:\n",
    "- How accurate our models are at identifying MVP winners\n",
    "- Whether they tend to have more false positives or false negatives\n",
    "- Which model performs best overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c8c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation & Reporting\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_prob = model.predict_proba(X_te)[:, 1]\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(classification_report(y_te, y_pred, digits=3))\n",
    "    print(f\"ROC AUC: {roc_auc_score(y_te, y_prob):.3f}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(y_te, y_pred).ravel()\n",
    "    print(f\"Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9049b7",
   "metadata": {},
   "source": [
    "### ROC Curve Visualization\n",
    "\n",
    "This code creates ROC (Receiver Operating Characteristic) curves to visualize model performance:\n",
    "\n",
    "- ROC curves plot the True Positive Rate against the False Positive Rate\n",
    "- The diagonal line represents random chance (AUC = 0.5)\n",
    "- Curves closer to the top-left corner indicate better performance\n",
    "- AUC (Area Under Curve) provides a single metric to compare models:\n",
    "  - AUC = 1.0: Perfect classification\n",
    "  - AUC > 0.9: Excellent model\n",
    "  - AUC > 0.8: Good model\n",
    "  - AUC > 0.7: Fair model\n",
    "  - AUC < 0.6: Poor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure()\n",
    "for name, model in models.items():\n",
    "    probas = model.predict_proba(X_te)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_te, probas)\n",
    "    auc = roc_auc_score(y_te, probas)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves for MVP Prediction Models\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef978ae2",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis\n",
    "\n",
    "This code visualizes which features are most important for each model:\n",
    "\n",
    "- **Random Forest**: Uses native feature_importances_ based on mean decrease in impurity\n",
    "- **XGBoost**: Uses gain-based importance, showing how much each feature improves the model\n",
    "- **Logistic Regression**: Skipped as it doesn't have native feature importance (will use SHAP later)\n",
    "\n",
    "The horizontal bar charts show:\n",
    "- Which statistics most strongly predict MVP status\n",
    "- How feature importance varies between different model types\n",
    "- Potential insights for basketball analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb6056",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    if name == \"XGBoost\":\n",
    "        booster = model.get_booster()\n",
    "        score = booster.get_score(importance_type=\"gain\")\n",
    "        importances = pd.Series(score).reindex(features).fillna(0)\n",
    "    elif hasattr(model, \"feature_importances_\"):\n",
    "        importances = pd.Series(model.feature_importances_, index=features)\n",
    "    else:\n",
    "        print(f\"Skipping {name} (no native feature importance)\")\n",
    "        continue\n",
    "\n",
    "    importances = importances.sort_values(ascending=False)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    importances.plot(kind=\"barh\")\n",
    "    plt.title(f\"{name} Feature Importance\")\n",
    "    plt.xlabel(\"Importance Score\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be649616",
   "metadata": {},
   "source": [
    "### SHAP Analysis for Logistic Regression\n",
    "\n",
    "This code uses SHAP (SHapley Additive exPlanations) to explain the Logistic Regression model:\n",
    "\n",
    "- SHAP values show how each feature impacts model predictions\n",
    "- First, the input data is scaled using the same scaler used during training\n",
    "- The summary plot shows:\n",
    "  - Features ordered by importance (top to bottom)\n",
    "  - How feature values (red = high, blue = low) affect predictions\n",
    "  - The magnitude of impact each feature has on the model output\n",
    "\n",
    "This helps explain which statistics most strongly influence MVP predictions in the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP interpretability for Logistic Regression\n",
    "scaler = models[\"LogisticRegression\"].named_steps[\"scaler\"]\n",
    "estimator = models[\"LogisticRegression\"].named_steps[\"lr\"]\n",
    "X_scaled = scaler.transform(X_tr)\n",
    "\n",
    "explainer = shap.Explainer(estimator.predict, masker=shap.maskers.Independent(X_scaled))\n",
    "shap_vals = explainer(X_scaled)\n",
    "shap.summary_plot(\n",
    "    shap_vals, \n",
    "    X_scaled, \n",
    "    feature_names=features, \n",
    "    show=False\n",
    ")\n",
    "plt.title(\"SHAP Summary — Logistic Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37198a",
   "metadata": {},
   "source": [
    "### SHAP Analysis for Random Forest\n",
    "\n",
    "This code applies SHAP analysis to the Random Forest model:\n",
    "\n",
    "- Uses TreeExplainer, which is optimized for tree-based models\n",
    "- Specifically looks at the positive class (MVP=1) to understand what drives MVP predictions\n",
    "- Shows how different feature values push predictions toward or away from MVP status\n",
    "\n",
    "The summary plot reveals:\n",
    "- Non-linear relationships captured by the Random Forest\n",
    "- Interaction effects between features\n",
    "- How the model's decision-making differs from Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe65c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random Forest SHAP ---\n",
    "explainer_rf = shap.TreeExplainer(models[\"RandomForest\"])\n",
    "shap_vals_rf_all = explainer_rf.shap_values(X_scaled)  \n",
    "\n",
    "# pick out class 1(MVP) across the third axis, shows how each feature pushes the model toward predicting MVP\n",
    "shap_vals_rf = shap_vals_rf_all[:, :, 1]  \n",
    "\n",
    "# now shap_vals_rf has shape (n_samples, n_features), so it will line up\n",
    "shap.summary_plot(\n",
    "    shap_vals_rf,\n",
    "    X_scaled,\n",
    "    feature_names=features,\n",
    "    show=False\n",
    ")\n",
    "plt.title(\"SHAP Summary — Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39594e",
   "metadata": {},
   "source": [
    "### SHAP Analysis for XGBoost\n",
    "\n",
    "This code creates SHAP visualizations for the XGBoost model:\n",
    "\n",
    "- XGBoost often captures complex relationships and interactions\n",
    "- The summary plot shows how each feature contributes to predictions\n",
    "- Color indicates feature value (red = high, blue = low)\n",
    "\n",
    "Comparing this to the other SHAP plots helps identify:\n",
    "- Consistent patterns across different model types\n",
    "- Unique insights that only XGBoost captures\n",
    "- The most robust predictors of MVP status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca64c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost SHAP ---\n",
    "explainer_xgb = shap.TreeExplainer(models[\"XGBoost\"])\n",
    "shap_vals_xgb = explainer_xgb(X_scaled)\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_vals_xgb,\n",
    "    X_scaled,\n",
    "    feature_names=features,\n",
    "    show=False\n",
    ")\n",
    "plt.title(\"SHAP Summary — XGBoost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34b453",
   "metadata": {},
   "source": [
    "### Cross-Validation Results\n",
    "\n",
    "This code implements 5-fold stratified cross-validation to assess model stability:\n",
    "\n",
    "- Stratified K-Fold ensures each fold maintains the class distribution\n",
    "- ROC AUC score evaluates performance on each fold\n",
    "- The mean and standard deviation of scores show:\n",
    "  - Overall model performance across different data subsets\n",
    "  - Model stability (lower standard deviation = more stable)\n",
    "  - Whether any model might be overfitting\n",
    "\n",
    "Cross-validation provides a more robust evaluation than a single train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa48bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"=== Cross-Validation Results ===\")\n",
    "for name, model in models.items():\n",
    "    if name == \"LogisticRegression\":\n",
    "        X_input = X_tr\n",
    "        model_cv = model\n",
    "    else:\n",
    "        X_input = X_tr\n",
    "        model_cv = model\n",
    "\n",
    "    scores = cross_val_score(model_cv, X_input, y_tr, cv=cv, scoring=\"roc_auc\")\n",
    "    print(f\"{name}: Mean ROC AUC = {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a160bc98",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "This code performs exhaustive hyperparameter tuning for each model:\n",
    "\n",
    "1. **Logistic Regression**:\n",
    "   - Tests different regularization types (L1, L2) and strengths\n",
    "   - Uses F1-macro score for imbalanced class optimization\n",
    "\n",
    "2. **Random Forest**:\n",
    "   - Tunes tree count, depth, min_samples_split, and class weighting\n",
    "   - Balances complexity vs. performance\n",
    "\n",
    "3. **XGBoost**:\n",
    "   - Optimizes estimators, depth, and learning rate\n",
    "   - Finds the right balance of model complexity\n",
    "\n",
    "GridSearchCV uses 3-fold cross-validation to find the best parameters for each model type, helping improve model performance while reducing overfitting risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid_log = {\n",
    "    \"logreg__penalty\": [\"l2\", \"l1\"],\n",
    "    \"logreg__C\": [0.01, 0.1, 1, 10],\n",
    "}\n",
    "logreg_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression(max_iter=9000, solver=\"saga\"))\n",
    "])\n",
    "grid_log = GridSearchCV(logreg_pipeline, param_grid_log, cv=3, scoring=\"f1_macro\", n_jobs=-1)\n",
    "grid_log.fit(X_tr, y_tr)\n",
    "\n",
    "best_logreg = grid_log.best_estimator_\n",
    "print(\"Best Logistic Regression Params:\", grid_log.best_params_)\n",
    "\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [5, 10, None],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"class_weight\": [\"balanced\"]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=3, scoring=\"f1_macro\", n_jobs=-1)\n",
    "grid_rf.fit(X_tr, y_tr)\n",
    "\n",
    "best_rf = grid_rf.best_estimator_\n",
    "print(\"Best Random Forest Params:\", grid_rf.best_params_)\n",
    "\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [3, 6],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(XGBClassifier(eval_metric=\"mlogloss\"), param_grid_xgb, cv=3, scoring=\"f1_macro\", n_jobs=-1)\n",
    "grid_xgb.fit(X_tr, y_tr)\n",
    "\n",
    "best_xgb = grid_xgb.best_estimator_\n",
    "print(\"Best XGBoost Params:\", grid_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b162d5",
   "metadata": {},
   "source": [
    "## Temporal Evaluation\n",
    "\n",
    "This code evaluates model performance across different time periods:\n",
    "\n",
    "1. **Train-Test Temporal Split**:\n",
    "   - Training data: Seasons up to and including 2010\n",
    "   - Testing data: Seasons after 2010\n",
    "   - This tests how well models trained on historical data predict recent MVPs\n",
    "\n",
    "2. **Per-Season Analysis**:\n",
    "   - Calculates F1 scores for each model on each NBA season\n",
    "   - Helps identify if certain seasons are more difficult to predict\n",
    "   - Shows temporal trends in model performance\n",
    "\n",
    "3. **Visualization**:\n",
    "   - Line chart plotting F1 scores by season\n",
    "   - Shows how prediction accuracy changes over time\n",
    "   - Helps identify whether model performance is stable or deteriorating\n",
    "\n",
    "This temporal validation approach is more realistic than random splitting since it mimics how we would actually use the model: training on past data to predict future MVPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c6dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_df = df[df[\"season\"] <= 2010]\n",
    "test_df = df[df[\"season\"] > 2010]\n",
    "X_test_aligned = test_df[features].copy()\n",
    "y_test = test_df[\"MVP\"]\n",
    "\n",
    "# Add predictions from each model\n",
    "test_df = test_df.copy()\n",
    "test_df[\"MVP\"] = y_test \n",
    "test_df[\"y_pred_log\"] = best_logreg.predict(X_test_aligned)\n",
    "test_df[\"y_pred_rf\"] = best_rf.predict(X_test_aligned)\n",
    "test_df[\"y_pred_xgb\"] = best_xgb.predict(X_test_aligned)\n",
    "\n",
    "# Evaluate F1 scores per season\n",
    "season_results = (\n",
    "    test_df.groupby(\"season\", group_keys=False)\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"F1 (LogReg)\": f1_score(g[\"MVP\"], g[\"y_pred_log\"], average=\"macro\"),\n",
    "        \"F1 (RF)\": f1_score(g[\"MVP\"], g[\"y_pred_rf\"], average=\"macro\"),\n",
    "        \"F1 (XGB)\": f1_score(g[\"MVP\"], g[\"y_pred_xgb\"], average=\"macro\"),\n",
    "    }))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(season_results)\n",
    "season_results.plot(x=\"season\", marker=\"o\")\n",
    "plt.title(\"F1 Score by Season (Temporal Evaluation)\")\n",
    "plt.ylabel(\"F1 Score (Macro)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6025f1",
   "metadata": {},
   "source": [
    "## Final Model Comparison\n",
    "\n",
    "This code creates a final performance summary of our best models:\n",
    "\n",
    "- Uses the hyperparameter-tuned versions of each model\n",
    "- Evaluates on the test set (seasons after 2010)\n",
    "- Compares key performance metrics:\n",
    "  - **Accuracy**: Overall proportion of correct predictions\n",
    "  - **F1 Score (Macro)**: Balanced measure of precision and recall\n",
    "\n",
    "The summary table provides a clear, side-by-side comparison to determine which model:\n",
    "- Most accurately identifies MVP winners\n",
    "- Best balances false positives and false negatives\n",
    "- Would be most suitable for real-world MVP predictions\n",
    "\n",
    "This final comparison helps us select the best model for deployment or further refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9467b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "X_eval = test_df[features]\n",
    "y_eval = test_df[\"MVP\"] \n",
    "\n",
    "summary = []\n",
    "\n",
    "models_to_compare = {\n",
    "    \"Logistic Regression\": best_logreg,\n",
    "    \"Random Forest\": best_rf,\n",
    "    \"XGBoost\": best_xgb\n",
    "}\n",
    "\n",
    "for name, model in models_to_compare.items():\n",
    "    y_pred = model.predict(X_eval)\n",
    "    y_proba = model.predict_proba(X_eval)\n",
    "\n",
    "    acc = accuracy_score(y_eval, y_pred)\n",
    "    f1 = f1_score(y_eval, y_pred, average=\"macro\")\n",
    "\n",
    "    summary.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": round(acc, 3),\n",
    "        \"F1 Score (Macro)\": round(f1, 3),\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(summary)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
