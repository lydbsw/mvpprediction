#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
mvp_prediction.py

Predict NBA MVP winners (1980–2024) without PER, with expanded reporting:
  • For each model: classification report, ROC AUC, confusion matrix
  • List of all model‐predicted MVPs
  • For each real MVP season: actual MVP + model’s single top‐probability pick (p=…)
  • SHAP interpretability for Logistic Regression
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import shap

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    confusion_matrix
)


def load_data():
    mvp = pd.read_csv("data/mvp_history.csv", skiprows=1, usecols=["Season", "Player"])
    mvp["season"] = mvp["Season"].str.split("-", expand=True)[0].astype(int) + 1
    mvp = (
        mvp.rename(columns={"Player": "player"})
           .loc[:, ["season", "player"]]
           .assign(MVP=1)
    )

    bb = pd.read_csv("data/NBA_Dataset.csv")

    rap_h = pd.read_csv("data/historical_RAPTOR_by_player.csv")
    rap_m = pd.read_csv("data/modern_RAPTOR_by_player.csv")
    rap_cols = ["player_name", "season", "raptor_offense", "raptor_defense", "raptor_total"]
    rap = (
        pd.concat([rap_h[rap_cols], rap_m[rap_cols]], ignore_index=True)
           .drop_duplicates(subset=["player_name", "season"])
           .rename(columns={"player_name": "player"})
    )

    return mvp, bb, rap


def preprocess_and_merge(mvp, bb, rap):
    df = (
        bb.merge(rap, on=["season", "player"], how="left")
          .merge(mvp, on=["season", "player"], how="left")
    )
    df["MVP"] = df["MVP"].fillna(0).astype(int)
    num_cols = df.select_dtypes(include=[np.number]).columns
    df[num_cols] = df[num_cols].fillna(df[num_cols].median())
    return df


def feature_engineering(df):
    feats = [
        'ws', 'bpm', 'vorp', 'win_loss_pct',
        'pts_per_g', 'trb_per_g', 'ast_per_g',
        'raptor_offense', 'raptor_defense', 'raptor_total'
    ]
    return df[feats], df["MVP"]


def train_models(X_train, y_train):
    models = {}

    # Logistic Regression
    pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("lr", LogisticRegression(max_iter=2000, random_state=42))
    ])
    pipe.fit(X_train, y_train)
    models["LogisticRegression"] = pipe

    # Random Forest
    rf = RandomForestClassifier(
        n_estimators=300, max_depth=8, random_state=42, n_jobs=-1
    )
    rf.fit(X_train, y_train)
    models["RandomForest"] = rf

    # XGBoost
    xg = xgb.XGBClassifier(
        n_estimators=300,
        learning_rate=0.05,
        use_label_encoder=False,
        eval_metric="logloss",
        random_state=42,
        n_jobs=-1
    )
    xg.fit(X_train, y_train)
    models["XGBoost"] = xg

    return models


def evaluate_and_report(models, df_test, X_test, y_test):
    # 1) Print metrics + list of predicted MVPs per model
    for name, model in models.items():
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

        print(f"\n=== {name} ===")
        print(classification_report(y_test, y_pred, digits=3))
        print(f"ROC AUC: {roc_auc_score(y_test, y_prob):.3f}")
        print(f"Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}")

        df_test[f"{name}_pred"] = y_pred
        df_test[f"{name}_prob"] = y_prob

        preds = df_test[df_test[f"{name}_pred"] == 1][["season", "player", f"{name}_prob"]]
        print(f"\n{name} predicted MVPs ({len(preds)}):")
        print(preds.to_string(index=False))

    # 2) No “True vs. Predicted” block

    # 3) For each real MVP season, show actual MVP + model’s top‑probability pick:
    actual_mvps = df_test[df_test["MVP"] == 1]
    print("\n--- Predictions on Actual MVP Seasons ---")
    for _, row in actual_mvps.iterrows():
        season = row["season"]
        actual = row["player"]
        print(f"\nSeason {season} — Actual MVP: {actual}")

        # All test rows for that season
        season_df = df_test[df_test["season"] == season]

        for name in models:
            # pick the row with max probability
            top = season_df.loc[season_df[f"{name}_prob"].idxmax()]
            pick_player = top["player"]
            pick_prob   = top[f"{name}_prob"]
            print(f"  • {name} predicted: {pick_player} (p={pick_prob:.3f})")


def shap_summary(model_pipeline, X_ref, out="shap_summary.png"):
    scaler = model_pipeline.named_steps["scaler"]
    estimator = model_pipeline.named_steps["lr"]
    X_scaled = scaler.transform(X_ref)

    explainer = shap.Explainer(
        estimator.predict,
        masker=shap.maskers.Independent(X_scaled)
    )
    shap_vals = explainer(X_scaled)

    shap.summary_plot(
        shap_vals,
        X_scaled,
        feature_names=X_ref.columns,
        show=False
    )
    plt.tight_layout()
    plt.savefig(out)
    print(f"\nSHAP summary plot saved to {out}")


def main():
    mvp, bb, rap = load_data()
    df = preprocess_and_merge(mvp, bb, rap)

    X, y = feature_engineering(df)

    idx = df.index.to_numpy()
    X_tr, X_te, y_tr, y_te, idx_tr, idx_te = train_test_split(
        X, y, idx,
        test_size=0.20,
        stratify=y,
        random_state=42
    )

    df_test = df.loc[idx_te, ["season", "player", "MVP"]].reset_index(drop=True)

    models = train_models(X_tr, y_tr)
    evaluate_and_report(models, df_test, X_te, y_te)
    shap_summary(models["LogisticRegression"], X_tr)


if __name__ == "__main__":
    main()
